<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[OpenResty Best Practice]]></title>
    <url>%2F2018%2F10%2F31%2FOpenResty-Best-Practice%2F</url>
    <content type="text"><![CDATA[目录 OpenResty 发展起源 OpenResty 之 lua 编程 OpenResty 模块编写 OpenResty 核心原理 OpenResty hooks OpenResty 开发常见陷阱 OpenResty 编程优化 OpenResty 易混易错配置解析 nginx 维护与更新 OpenResty 发展起源 OpenResty(也称为 ngx_openresty)是一个全功能的 Web 应用服务器。它打包了标准的 nginx 核心，很多的常用的第三方模块，以及它们的大多数依赖项。通过揉和众多设计良好的 nginx 模块，OpenResty 有效地把 nginx 服务器转变为一个强大的 Web 应用服务器，基于它开发人员可以使用 lua 编程语言对 nginx 核心以及现有的各种 nginx C 模块进行脚本编程，构建出可以处理一万以上并发请求的极端高性能的 Web 应用。 OpenResty 致力于将你的服务器端应用完全运行于 nginx 服务器中，充分利用 nginx 的事件模型来进行非阻塞 I/O 通信。不仅仅是和 HTTP 客户端间的网络通信是非阻塞的，与 MySQL、PostgreSQL、Memcached 以及 Redis 等众多后端之间的网络通信也是非阻塞的。因为 OpenResty 软件包的维护者也是其中打包的许多 nginx 模块的作者，所以 OpenResty 可以确保所包含的所有组件可以可靠地协同工作。 OpenResty 最早是雅虎中国的一个公司项目，起步于 2007 年 10 月。当时兴起了 OpenAPI 的热潮，用于满足各种 Web Service 的需求，基于 Perl 和 Haskell 实现；2009 章亦春在加入淘宝数据部门的量子团队，决定对 OpenResty 进行重新设计和彻底重写，并把应用重点放在支持像量子统计这样的 Web 产品上面，这是第二代的 OpenResty，基于 nginx 和 lua 进行开发。 为什么要取 OpenResty 这个名字呢？OpenResty 最早是顺应 OpenAPI 的潮流做的，所以 Open 取自“开放”之意，而 Resty 便是 REST 风格的意思。虽然后来也可以基于 ngx_openresty 实现任何形式的 Web service 或者传统的 Web 应用。 也就是说 nginx 不再是一个简单的静态网页服务器，也不再是一个简单的反向代理了，OpenResty 致力于通过一系列 nginx 模块，把 nginx 扩展为全功能的 Web 应用服务器，目前有两大应用目标： 通用目的的 Web 应用服务器。在这个目标下，现有的 Web 应用技术都可以算是和 OpenResty 或多或少有些类似，比如 Nodejs，PHP 等等，但 OpenResty 的性能更加出色。 nginx 的脚本扩展编程，为构建灵活的 Web 应用网关和 Web 应用防火墙等功能提供了极大的便利性。 OpenResty 特性概括如下： 基于 nginx 的 Web 服务器 打包 nginx 核心、常用的第三方模块及依赖项 使用 lua 对 nginx 进行脚本编程 充分利用 nginx 的事件模型进行非阻塞 I/O 通信 使用 lua 以同步方式进行异步编程 拓展后端通信方式 综合 OpenResty 的特性，它不仅具备 nginx 的负载均衡、反向代理及传统 http server 等功能，还可以利用 lua 脚本编程实现路由网关，实现访问认证、流量控制、路由控制及日志处理等多种功能；同时利用 cosocket 拓展和后端(mysql、redis、kafaka)通信后，更可开发通用的 restful api 程序。 OpenResty 之 lua 编程 lua 简介1993 年在巴西里约热内卢天主教大学诞生了一门编程语言，他们给这门语言取了个浪漫的名字 — lua，在葡萄牙语里代表美丽的月亮。事实证明他们没有糟蹋这个优美的单词，lua 语言正如它名字所预示的那样成长为一门简洁、优雅且富有乐趣的语言。 lua 从一开始就是作为一门方便嵌入(其它应用程序)并可扩展的轻量级脚本语言来设计，因此她一直遵从着简单、小巧、可移植、快速的原则，官方实现完全采用 ANSI C 编写，能以 C 程序库的形式嵌入到宿主程序中。luaJIT 2 和标准 lua 5.1 解释器采用的是著名的 MIT 许可协议。正由于上述特点，所以 lua 在游戏开发、机器人控制、分布式应用、图像处理、生物信息学等各种各样的领域中得到了越来越广泛的应用。其中尤以游戏开发为最，许多著名的游戏，比如 World of Warcraft、大话西游，都采用了 lua 来配合引擎完成数据描述、配置管理和逻辑控制等任务。即使像 Redis 这样中性的内存键值数据库也提供了内嵌用户 lua 脚本的官方支持。 作为一门过程型动态语言，lua 有着如下的特性： 变量名没有类型，值才有类型，变量名在运行时可与任何类型的值绑定； 语言只提供唯一一种数据结构，称为表(table)，它混合了数组、哈希，可以用任何类型的值作为 key 和 value。提供了一致且富有表达力的表构造语法，使得 lua 很适合描述复杂的数据； 函数是一等类型，支持匿名函数和正则尾递归(proper tail recursion)； 支持词法定界(lexical scoping)和闭包(closure)； 提供 thread 类型和结构化的协程(coroutine)机制，在此基础上可方便实现协作式多任务； 运行期能编译字符串形式的程序文本并载入虚拟机执行； 通过元表(metatable)和元方法(metamethod)提供动态元机制(dynamic meta-mechanism)，从而允许程序运行时根据需要改变或扩充语法设施的内定语义； 能方便地利用表和动态元机制实现基于原型(prototype-based)的面向对象模型； 从 5.1 版开始提供了完善的模块机制，从而更好地支持开发大型的应用程序； lua 基础数据类型12345print(type(&quot;hello world&quot;)) --&gt; output:stringprint(type(print)) --&gt; output:functionprint(type(true)) --&gt; output:booleanprint(type(360.0)) --&gt; output:numberprint(type(nil)) --&gt; output:nil nilnil 是一种类型，lua 将 nil 用于表示“无效值”。一个变量在第一次赋值前的默认值是 nil，将 nil 赋予给一个全局变量就等同于删除它。 12345local numprint(num) --&gt; output:nilnum = 100print(num) --&gt; output:100 boolean (true/false)布尔类型，可选值 true/false；lua 中 nil 和 false 为“假”，其它所有值均为“真”，比如 0 和空字符串就是“真”。 123456789101112131415161718192021local a = truelocal b = 0local c = nilif a then print(&quot;a&quot;) --&gt; output:aelse print(&quot;not a&quot;) -- 这个没有执行endif b then print(&quot;b&quot;) --&gt; output:belse print(&quot;not b&quot;) -- 这个没有执行endif c then print(&quot;c&quot;) -- 这个没有执行else print(&quot;not c&quot;) --&gt; output:not cend numberNumber 类型用于表示实数，和 C/C++ 里面的 double 类型很类似。可以使用数学函数 math.floor(向下取整)和 math.ceil(向上取整)进行取整操作。 1234local order = 3.99local score = 98.01print(math.floor(order)) --&gt; output:3print(math.ceil(score)) --&gt; output:99 string和其他语言 string 大同小异 123456789local str1 = &apos;hello world&apos;local str2 = &quot;hello lua&quot;local str3 = [[&quot;add\name&quot;,&apos;hello&apos;]]local str4 = [=[string have a [[]].]=]print(str1) --&gt; output:hello worldprint(str2) --&gt; output:hello luaprint(str3) --&gt; output:&quot;add\name&quot;,&apos;hello&apos;print(str4) --&gt; output:string have a [[]]. table (数组、字典)Table 类型实现了一种抽象的“关联数组”。“关联数组”是一种具有特殊索引方式的数组，索引通常是字符串(string)或者 number 类型，但也可以是除 nil 以外的任意类型的值。 123456789101112131415161718local corp = &#123; web = &quot;www.google.com&quot;, -- 索引为字符串，key = &quot;web&quot;, -- value = &quot;www.google.com&quot; telephone = &quot;12345678&quot;, -- 索引为字符串 staff = &#123;&quot;Jack&quot;, &quot;Scott&quot;, &quot;Gary&quot;&#125;, -- 索引为字符串，值也是一个表 100876, -- 相当于 [1] = 100876，此时索引为数字 -- key = 1, value = 100876 100191, -- 相当于 [2] = 100191，此时索引为数字 [10] = 360, -- 直接把数字索引给出 [&quot;city&quot;] = &quot;Beijing&quot; -- 索引为字符串&#125;print(corp.web) --&gt; output:www.google.comprint(corp[&quot;telephone&quot;]) --&gt; output:12345678print(corp[2]) --&gt; output:100191print(corp[&quot;city&quot;]) --&gt; output:&quot;Beijing&quot;print(corp.staff[1]) --&gt; output:Jackprint(corp[10]) --&gt; output:360 在内部实现上，table 通常实现为一个哈希表、一个数组、或者两者的混合。具体的实现为何种形式，动态依赖于具体的 table 的键分布特点。 function在 lua 中，函数也是一种数据类型，函数可以存储在变量中，可以通过参数传递给其他函数，还可以作为其他函数的返回值。 123456789101112131415local function foo() print(&quot;in the function&quot;) -- dosomething() local x = 10 local y = 20 return x + yendlocal a = foo -- 把函数赋给变量print(a())-- output:in the function30 lua 表达式 算术运算符 说明 关系运算符 说明 逻辑运算符 说明 + 加法 &lt; 小于 and 逻辑与 - 减法 &gt; 大于 or 逻辑或 * 乘法 &lt;= 小于等于 not 逻辑非 / 除法 &gt;= 大于等于 - - ^ 指数 ~= 不等于 - - % 取模 - - - - note: lua 中的不等于用 ~= 表示， 和其他语言的 != 不一致 lua 流程控制lua 的流程控制结构和 python 类似，有几个特例: lua 中的 elseif 需要连写，中间不能有空行；python 中写法是 elif lua 中没有 continue 流控 if/else/elseif1234567if a = 1 then print(&quot;1&quot;)elseif a == 2 then print(&quot;2&quot;)else print(&quot;3&quot;)end while123456while a &gt; 1 do if a == 5 then break end a = a + 1end repeat1234567local i = 0repeat print(i) if i == 5 then break enduntil true for/break12345678910111213local t = &#123; a = 1, b = 2&#125;for k, v in pairs(t) do -- 遍历字典 print(k, v)endlocal t = &#123;1, 2&#125;for k, v in ipairs(t) do -- 遍历整型数组 print(k, v)endfor i = 1, 10 do -- range 循环 print(i)end return1234567local function foo(arg) if arg == &quot;&quot; then return nil end return &quot;bar&quot;end OpenResty 模块编写 编写一个 access.lua 模块，源码如下：123456789local _M = &#123;&#125;_M.check = function() if ngx.var.http_host == &quot;foo.bar.com&quot; then ngx.exit(403) endendreturn _M -- 注意 return _M，返回 table 表示的模块 在 access_by_lua 的 nginx hook 中调用 access 模块：1234access_by_lua_block &#123; local rule = require &quot;access&quot; -- require 中不需要加 `.lua` 后缀 rule.check()&#125; OpenResty 核心原理 nginx 进程模型nginx 是一个 master + 多个 worker 进程模型；master 进程负责管理和监控 worker 进程，如加载和解析配置文件，重启 worker 进程，更新二进制文件等。 worker 进程负责处理请求，每个 worker 地位和功能相同，内部按照 epoll + callback 方式实现并发连接处理；整体架构图如下： nginx 请求处理流程每个 worker 进程都分阶段处理 http 请求，简单概括为初始化请求 -&gt; 处理请求行 -&gt; 后端交互 -&gt; 响应头处理 -&gt; 响应包体处理 -&gt; 打印日志等几个阶段。其中处理响应体阶段又可以挂载多个不同的 filter。具体的请求阶段可以参见nginx Phase, nginx 请求处理流程如下图： nginx 事件机制nginx 的事件驱动机制是对 epoll 驱动的封装，但其本质还是 epoll + callback 方式： lua 协程 函数 描述 coroutine.create() 创建 coroutine，返回 coroutine，参数是一个函数，当和 resume 配合使用的时候就唤醒函数调用 coroutine.resume() 重启 coroutine，和 create 配合使用 coroutine.yield() 挂起 coroutine，将 coroutine 设置为挂起状态，这个和 resume 配合使用能有很多有用的效果 coroutine.status() 查看 coroutine 的状态。注：coroutine 的状态有四种：dead，suspend，running，normal coroutine.create(f)创建一个主体函数为 f 的新协程。f 必须是一个 lua 的函数。返回这个新协程，它是一个类型为 “thread” 的对象，创建后并不会启动该协程。 coroutine.resume(co, [, val1, …])开始或继续协程 co 的运行。当第一次执行一个协程时，他会从主函数处开始运行。val1, … 这些值会以参数形式传入主体函数。如果该协程被挂起，resume 会重新启动它；val1, … 这些参数会作为挂起点的返回值。如果协程运行起来没有错误，resume 返回 true 加上传给 yield 的所有值 (当协程挂起)，或是主体函数的所有返回值(当协程中止)。 coroutine.yield(…)挂起正在调用的协程的执行。 传递给 yield 的参数都会转为 resume 的额外返回值。 coroutine.status(co)以字符串形式返回协程 co 的状态： 当协程正在运行(它就是调用 status 的那个) ，返回 “running”； 如果协程调用 yield 挂起或是还没有开始运行，返回 “suspended”； 如果协程是活动的，都并不在运行(即它正在延续其它协程)，返回 “normal”； 如果协程运行完主体函数或因错误停止，返回 “dead”。 协程实例(生产者消费者)使用协程实现生产者消费者：1234567891011121314151617181920212223local function produce() while true do local x = io.read() coroutine.yield(x) -- 挂起协程 endendlocal producer = coroutine.create(produce) -- 创建协程local function receive() local status, value = coroutine.resume(producer) -- 执行协程 return valueendlocal function consumer() while true do local x = receive() io.write(x, &quot;\n&quot;) endendconsumer() -- loop lua 与 c 堆栈交互lua 虚拟机常嵌入 C 程序中运行，对于 C 程序来说，lua 虚拟机就是一个子进程。lua 将所有状态都保存在 lua_State 类型中，所有的 C API 都要求传入一个指向该结构的指针。我们根据这个指针来获取 lua 虚拟机(也就是子进程)的状态。 虚拟机内部与外部的 C 程序发生数据交换主要是通过一个公用栈实现的，也就是说 lua 虚拟机和 C 程序公用一个栈，双方都可以压栈或读取数据。一方压入，另一方弹出就能实现数据的交换。 在 c 中，lua 堆栈就是一个 struct，堆栈索引方式可能是正数也可能是负数，区别是：正数索引 1 永远表示栈底，负数索引 -1 永远表示栈顶。堆栈的默认大小是 20，可以用 lua_checkstack 修改，用 lua_gettop 则可以获得栈里的元素数目。 C 调用 lua 在 C 中创建 lua 虚拟机 1lua_State *luaL_newstate (void) 加载 lua 的库函数 1void luaL_openlibs (lua_State *L); 加载 lua 文件，使用接口 1int luaL_dofile (lua_State *L, const char *filename); 开始交互，lua 定义一个函数 1function test_func_add(a, b) return a + b end 如果你的 lua_State 是全局变量，那么每次对堆栈有新操作时务必使用lua_settop(lua_State, -1)将偏移重新置到栈顶 去lua文件中取得test_func_add方法 1void lua_getglobal (lua_State *L, const char *name); 参数压栈 1lua_pushnumber 通过 pcall 调用 1int lua_pcall (lua_State *L, int nargs, int nresults, int msg); 完整示例，先编写一个 foo.lua 文件，在文件中实现 test_func_add 方法123function test_func_add(a, b) return a + bend 接下来在 C 代码中调用 foo.lua:12345678910111213141516171819202122232425262728293031323334353637383940414243444546lua_State* init_lua()&#123; lua_State* s_lua = luaL_newstate(); if (!s_lua) &#123; printf(&quot;luaL_newstate failed!\n&quot;); exit(-1); &#125; luaL_openlibs(s_lua); return s_lua;&#125;bool load_lua_file(lua_State* s_lua, const char* lua_file)&#123; if (luaL_dofile(s_lua, lua_file) != 0) &#123; printf(&quot;LOAD LUA %s %s\n&quot;, lua_file, BOOT_FAIL); return false; &#125; printf(&quot;LOAD LUA %s %s\n&quot;, lua_file, BOOT_OK); return true;&#125;int proc_add_operation(lua_State* s_lua, int a, int b)&#123; lua_settop(s_lua, -1); lua_getglobal(s_lua, &quot;test_func_add&quot;); lua_pushnumber(s_lua, a); lua_pushnumber(s_lua, b); int val = lua_pcall(s_lua, 2, 1, 0); if (val) &#123; printf(&quot;lua_pcall_error %d\n&quot;, val); &#125; return (int)lua_tonumber(s_lua, -1);&#125;int main() &#123; lua_State* s_lua =init_lua(); if (!load_lua_file(s_lua, &quot;foo&quot;)) &#123; return -1; &#125; proc_add_operation(s_lua, 1, 2);&#125; lua 调用 c 定义谁先实现 C 接口 12345678#define target 300static int l_test_check_value(lua_State * l)&#123; int num = lua_tointeger(l, -1); bool check = (num == target); lua_pushboolean(l, check); return 1;&#125; lua 虚拟启动时候，注册加载 C 接口 1lua_register(s_lua, &quot;test_check_value&quot;, l_test_check_value); 在 lua 代码中调用注册的 C 接口 1234function test_func_check(a) local val = test_check_value(a) return valend lua 协程与 nginx 事件机制结合文章前部分用大量篇幅阐述了 lua 和 nginx 的相关知识，包括 nginx 的进程架构，nginx 的事件循环机制，lua 协程，lua 协程如何与 C 实现交互；在了解这些知识之后，本节阐述 lua 协程是如何和 nginx 的事件机制协同工作。 从 nginx 的架构和事件驱动机制来看, nginx 的并发处理模型概括为：单 worker + 多连接 + epoll + callback。即每个 nginx worker 同时处理了大量连接，每个连接对应一个 http 请求，一个 http 请求对应 nignx 中的一个结构体(ngx_http_request_t):123456789101112131415struct ngx_http_request_s &#123; uint32_t signature; /* &quot;HTTP&quot; */ ngx_connection_t *connection; void **ctx; void **main_conf; void **srv_conf; void **loc_conf; ngx_http_event_handler_pt read_event_handler; ngx_http_event_handler_pt write_event_handler; ....&#125; 结构体中的核心成员为 ngx_connection_t *connection，其定义如下: 12345678910111213141516struct ngx_connection_s &#123; void *data; ngx_event_t *read; // epoll 读事件对应的结构体成员 ngx_event_t *write; // epoll 写事件对应的结构体成员 ngx_socket_t fd; // tcp 对应的 socket fd ngx_recv_pt recv; ngx_send_pt send; ngx_recv_chain_pt recv_chain; ngx_send_chain_pt send_chain; ngx_listening_t *listening; ...&#125; 从如上结构体可知，每个请求中对应的 ngx_connection_t 中的读写事件和 epoll 关联；nginx epoll 的事件处理核心代码如下： 123456789101112131415161718192021 ... events = epoll_wait(ep, event_list, (int) nevents, timer); for (i = 0; i &lt; events; i++) &#123; c = event_list[i].data.ptr; instance = (uintptr_t) c &amp; 1; c = (ngx_connection_t *) ((uintptr_t) c &amp; (uintptr_t) ~1); // epoll 获取激活事件，将事件转换成 ngx_connection_t ... rev = c-&gt;read; rev-&gt;handler(rev); ... wev = c-&gt;write; wev-&gt;handler(ev); ...&#125; nginx epoll loop 中调用 epoll_wait 获取 epoll 接管的激活事件，并通过 c 的指针强转，得到 ngx_connection_t 获取对应的连接和连接上对应的读写事件的回调函数，即通过 C 结构体变量成员之间的相关关联来串联请求和事件驱动，实现请求的并发处理；这里其实和高级语言的面向对象的写法如出一辙，只是模块和成员变量之间的获取方式的差异。 如果引入 lua 的协程机制，在 lua 代码中出现阻塞的时候，主动调用 coroutine.yield 将自身挂起，待阻塞操作恢复时，再将挂起的协程调用 coroutine.resume 恢复则可以避免在 lua 代码中写回调；而何时恢复协程可以交由 c 层面的 epoll 机制来实现，则可以实现事件驱动和协程之间的关联。现在我们只需要考虑，如何将 lua_State 封装的 lua land 和 C land 中的 epoll 机制融合在一起。 事实上 lua-nginx-module 确实是按照这种方式来处理协程与 nginx 事件驱动之间的关系，lua-nginx-module 为每个 nginx worker 生成了一个 lua_state 虚拟机，即每个 worker 绑定一个 lua 虚拟机，当需要 lua 脚本介入请求处理流程时，基于 worker 绑定的虚拟机创建 lua_coroutine 来处理逻辑，当阻塞发生、需要挂起时或者处理逻辑完成时挂起自己，等待下次 epoll 调度时再次唤醒协程执行。如下是 rewrite_by_lua 核心代码部分： 123456789101112131415161718192021tatic ngx_int_tngx_http_lua_rewrite_by_chunk(lua_State *L, ngx_http_request_t *r)&#123; co = ngx_http_lua_new_thread(r, L, &amp;co_ref); lua_xmove(L, co, 1); ngx_http_lua_get_globals_table(co); lua_setfenv(co, -2); ngx_http_lua_set_req(co, r); // 此处设置协程与 ngx_http_request_t 之间的关系 ... rc = ngx_http_lua_run_thread(L, r, ctx, 0); // 运行 lua 脚本处理 rewrite 逻辑 if (rc == NGX_ERROR || rc &gt; NGX_OK) &#123; return rc; &#125; ...&#125; 从上述代码片段中我们看到了协程与 ngx 请求之间的绑定关系，那么只要在 ngx_http_lua_run_thread 函数中（实际上是在 lua 脚本中）处理何时挂起 lua 的执行即可。大部分时候我们在 lua 中的脚本工作类型分两种，一种是基于请求信息的逻辑改写，一种是基于 tcp 连接的后端交互。逻辑改写往往不会发生 io 阻塞，即当前脚本很快执行完成后回到 C land，不需要挂起再唤醒的流程。而对于方式二，lua-nginx-module 提供了 cosocket api，它封装了 tcp api，并且会在合适的时候（coroutine.yield 的调用发生在 IO 异常，读取包体完毕，或者 proxy_buffers 已满等情形，具体的实现读者可以参考 ngx_http_lua_socket_tcp.c 源码）调用 coroutine.yield 方法 。 综上所述，结合lua 协程和 nginx 事件驱动机制，使用 OpenResty 可以使用 lua 脚本方便的扩展 nignx 的功能。 OpenResty hooks (编程钩子) init_by_lua该阶段主要用于预加载一些 lua 模块， 如加载全局 json 模块：require &#39;cjson.safe&#39;；设置全局的 lua_share_dict 等，并且可以利用操作系统的 copy-on-write 机制；reload nginx 会重新加载该阶段的代码。 init_worker_by_lua该阶段可用于为每个 worker 设置独立的定时器，设置心跳检查等。 rewrite_by_lua实际场景中应用最多的一个 hooks 之一，可用于请求重定向相关的逻辑，如改写 host 头，改写请求参数和请求路径等 access_by_lua该阶段可用于实现访问控制相关的逻辑，如动态限流、限速，防盗链等 content_by_lua该阶段用于生成 http 请求的内容，和 proxy_pass 指令冲突；二者在同一个阶段只能用一个。该阶段可用于动态的后端交互，如 mysql、redis、kafaka 等；也可用于动态的 http 内容生成，如使用 lua 实现 c 的 slice 功能，完成大文件的分片切割。 banalce_by_lua该阶段可用于动态的设置 proxy_pass 的上游地址，例如用 lua 实现一个带监控检测机制的一致性 hash 轮序后端算法，根据上游的响应动态设置该地址是否可用。 body_filter_by_lua用于过滤和加工响应包体，如对 chunk 模式的包体进行 gzip; 也可以根据包体的大小来动态设置 ngx.var.limit_rate. header_filter_by_lua调整发送给 client 端的响应头，也是最常用的 hooks 之一；比如设置响应的 server 头，修缓存头 cache-control 等。 log_by_lua一方面可以设置 nginx 日志输出的字段值，另一方面我们也可以用 cosocket 将日志信息发送到指定的 http server；因响应头和响应体已发送给客户端，该阶段的操作不会影响到客户端的响应速度。 OpenResty 之 lua 编写常见陷阱 elseif，区别于 else if； and &amp; or，不支持问号表达式；lua 中 0 表示 true； no continue，lua 中不支持 continue 语法；需要用 if 和 else 语句实现； . &amp; :，lua 中 object.method 和 object:method 行为不同，object:method 为语法糖，会扩展成第一个参数为 self forgot return _M，在编写模块的时候如果最后忘记 return _M, 调用时会提示尝试对 string 调用方法的异常 OpenResty 编程优化 do local statement，尽量使用 local 化的变量声明，加速变量索引速度的同时避免全局命名空间的污染； do not use blocked api，不要调用会阻塞 lua 协程的 api，比如 lua 原生的 socket，会造成 nginx worker block； use ngx.ctx instead of ngx.var，ngx.var 会调用 ngx.var 的变量索引系统，比 ngx.ctx 低效很多； decrease table resize，避免 lua table 表的 resize 操作，可以用 luajit 事先声明指定大小的 table。比如频繁的 lua 字符串相加的 .. 操作，当 lua 预分配内存不够时，会重新动态扩容(和 c++ vector 类型)，会造成低效； use lua-resty-core，使用 lua-resty-core api，该部分 api 用 luajit 的 ffi 实现比直接的 C 和 lua 交互高效； use jit support function，少用不可 jit 加速的函数，那些函数不能 jit 支持，可以参看 luajit 文档。 ffi，对自己实现的 C 接口，也建议用 ffi 暴露出接口给 lua 调用。 nginx 易混易错配置说明 so_keepalive用于 listen 中，探测连接保活; 采用TCP连接的C/S模式软件，连接的双方在连接空闲状态时，如果任意一方意外崩溃、当机、网线断开或路由器故障，另一方无法得知TCP连接已经失效，除非继续在此连接上发送数据导致错误返回。很多时候，这不是我们需要的。我们希望服务器端和客户端都能及时有效地检测到连接失效，然后优雅地完成一些清理工作并把错误报告给用户。 如何及时有效地检测到一方的非正常断开，一直有两种技术可以运用。一种是由TCP协议层实现的Keepalive，另一种是由应用层自己实现的心跳包。 TCP默认并不开启Keepalive功能，因为开启 Keepalive 功能需要消耗额外的宽带和流量，尽管这微不足道，但在按流量计费的环境下增加了费用，另一方面，Keepalive设置不合理时可能会因为短暂的网络波动而断开健康的TCP连接。并且，默认的Keepalive超时需要7,200,000 milliseconds，即2小时，探测次数为 5 次。系统默认的 keepalive 配置如下：123net.ipv4.tcpkeepaliveintvl = 75net.ipv4.tcpkeepaliveprobes = 5net.ipv4.tcpkeepalivetime = 7200 如果在 listen 的时候不设置 so_keepalive 则使用了系统默认的 keepalive 探测保活机制，需要 2 小时才能清理掉这种异常连接；如果在 listen 指令中加入1so_keepalive=30m::10 可设置如果连接空闲了半个小时后每 75s 探测一次，如果超过 10 次 探测失败，则释放该连接。 sendfile/directiosendfile copies data between one file descriptor and another. Because this copying is done within the kernel, sendfile() is more efficient than the combination of read(2) and write(2), which would require transferring data to and from user space. 从 Linux 的文档中可以看出，当 nginx 有磁盘缓存文件时候，可以利用 sendfile 特性将磁盘内容直接发送到网卡避免了用户态的读写操作。 directio Enables the use of the O_DIRECT flag (FreeBSD, Linux), the F_NOCACHE flag (macOS), or the directio() function (Solaris), when reading files that are larger than or equal to the specified size. The directive automatically disables (0.7.15) the use of sendfile for a given request 写文件时不经过 Linux 的文件缓存系统，不写 pagecache, 直接写磁盘扇区。启用aio时会自动启用directio, 小于directio定义的大小的文件则采用 sendfile 进行发送，超过或等于 directio 定义的大小的文件，将采用 aio 线程池进行发送，也就是说 aio 和 directio 适合大文件下载。因为大文件不适合进入操作系统的 buffers/cache,这样会浪费内存，而且 Linux AIO(异步磁盘IO) 也要求使用directio的形式。 proxy_request_buffering控制处理客户端包体的行为，如果设置为 on, 则 nginx 会接收完 client 的整个包体后处理。如 nginx 作为反向代理服务处理客户端的上传操作，则先接收完包体再转发给上游，这样上游异常的时候，nginx 可以多次重试上传，但有个问题是如果包体过大，nginx 端如果负载较重话，会有大量的写磁盘操作，同时对磁盘的容量也有较高要求。如果设置为 off, 则传输变成流式处理，一个 chunk 一个 chunk传输，传输出错更多需要 client 端重试。 proxy_buffer_size Sets the size of the buffer used for reading the first part of the response received from the proxied server. This part usually contains a small response header. By default, the buffer size is equal to one memory page. This is either 4K or 8K, depending on a platform. proxy_buffers Sets the number and size of the buffers used for reading a response from the proxied server, for a single connection. By default, the buffer size is equal to one memory page. This is either 4K or 8K, depending on a platform. proxy_buffering Enables or disables buffering of responses from the proxied server. When buffering is enabled, nginx receives a response from the proxied server as soon as possible, saving it into the buffers set by the proxy_buffer_size and proxy_buffers directives. If the whole response does not fit into memory, a part of it can be saved to a temporary file on the disk. Writing to temporary files is controlled by the proxy_max_temp_file_size and proxy_temp_file_write_size directives. When buffering is disabled, the response is passed to a client synchronously, immediately as it is received. nginx will not try to read the whole response from the proxied server. The maximum size of the data that nginx can receive from the server at a time is set by the proxy_buffer_size directive. 当 proxy_buffering on 时处理上游的响应可以使用 proxy_buffer_size 和 proxy_buffers 两个缓冲区；而设置 proxy_buffering off 时，只能使用proxy_buffer_size 一个缓冲区。 proxy_busy_size When buffering of responses from the proxied server is enabled, limits the total size of buffers that can be busy sending a response to the client while the response is not yet fully read. In the meantime, the rest of the buffers can be used for reading the response and, if needed, buffering part of the response to a temporary file. By default, size is limited by the size of two buffers set by the proxy_buffer_size and proxy_buffers directives. 当接收上游的响应发送给 client 端时，也需要一个缓存区，即发送给客户端而未确认的部分，这个 buffer 也是从 proxy_buffers 中分配，该指令限定能从 proxy_buffers 中分配的大小。 keepalive该指令可作用于 nginx.conf 和 upstream 的 server 中；当作用于 nginx.conf 中时，表示作为 http server 端回复客户端响应后，不关闭该连接，让该连接保持 ESTAB 状态，即 keepalive。 当该指令作用于 upstrem 块中时，表示发送给上游的 http 请求加入 connection: keepalive, 让服务端保活该连接。值得注意的是服务端和客户端均需要设置 keepalive 才能实现长连接。 同时 keepalive指令需要和 如下两个指令配合使用： 12keepalive_requests 100;keepalive_timeout 65; keepalive_requests 表示一个长连接可以复用的次数，keepalive_timeout 表示长连接在空闲多久后可以关闭。keepalive_timeout 如果设置过大会造成 nginx 服务端 ESTAB 状态的连接数增多。 nginx 维护与更新 nginx 信号集和 nginx 操作之间的对应关系如下： nginx operation signal reload SIGHUP reload SIGUSR1 stop SIGTERM quit SIGQUIT hot update SIGUSR2 &amp; SIGWINCH &amp; SIGQUIT stop vs quitstop 发送 SIGTERM 信号，表示要求强制退出，quit 发送 SIGQUIT，表示优雅地退出。 具体区别在于，worker 进程在收到 SIGQUIT 消息(注意不是直接发送信号，所以这里用消息替代)后，会关闭监听的套接字，关闭当前空闲的连接(可以被抢占的连接)，然后提前处理所有的定时器事件，最后退出。没有特殊情况，都应该使用 quit 而不是 stop。 reloadmaster 进程收到 SIGHUP 后，会重新进行配置文件解析、共享内存申请，等一系列其他的工作，然后产生一批新的 worker 进程，最后向旧的 worker 进程发送 SIGQUIT 对应的消息，最终无缝实现了重启操作。 再 master 进程重新解析配置文件过程中，如果解析失败则会回滚使用原来的配置文件，即 reload 失败，此时工作的还是老的 worker。 reopenmaster 进程收到 SIGUSR1 后，会重新打开所有已经打开的文件(比如日志)，然后向每个 worker 进程发送 SIGUSR1 信息，worker 进程收到信号后，会执行同样的操作。reopen 可用于日志切割，比如 nginx 官方就提供了一个方案： 1234$ mv access.log access.log.0$ kill -USR1 `cat master.nginx.pid`$ sleep 1$ gzip access.log.0 # do something with access.log.0 这里 sleep 1 是必须的，因为在 master 进程向 worker 进程发送 SIGUSR1 消息到 worker 进程真正重新打开 access.log 之间，有一段时间窗口，此时 worker 进程还是向文件 access.log.0 里写入日志的。通过 sleep 1s，保证了 access.log.0 日志信息的完整性(如果没有 sleep 而直接进行压缩，很有可能出现日志丢失的情况)。 hot update某些时候我们需要进行二进制热更新，nginx 在设计的时候就包含了这种功能，不过无法通过 nginx 提供的命令行完成，我们需要手动发送信号。 首先需要给当前的 master 进程发送 SIGUSR2，之后 master 会重命名 nginx.pid 到 nginx.pid.oldbin，然后 fork 一个新的进程，新进程会通过 execve 这个系统调用，使用新的 nginx ELF 文件替换当前的进程映像，成为新的 master 进程。新 master 进程起来之后，就会进行配置文件解析等操作，然后 fork 出新的 worker 进程开始工作。 接着我们向旧的 master 发送 SIGWINCH 信号，然后旧的 master 进程则会向它的 worker 进程发送 SIGQUIT 信息，从而使得 worker 进程退出。向 master 进程发送 SIGWINCH 和 SIGQUIT 都会使得 worker 进程退出，但是前者不会使得 master 进程也退出。 最后，如果我们觉得旧的 master 进程使命完成，就可以向它发送 SIGQUIT 信号，让其退出了。 引用 OpenResty 最佳实践 lua-nginx-module Readme nginx doc lua 与 c 进行交互 浅谈 nginx 信号集]]></content>
  </entry>
  <entry>
    <title><![CDATA[Linux 系统优化指令杂记]]></title>
    <url>%2F2018%2F09%2F24%2Flinux-system-profile-command%2F</url>
    <content type="text"><![CDATA[系统优化的前 5 分钟使用如下指令了解机器的整体运行情况, 此处有指令的中文解释 uptime 检查系统是否宕机重启 dmesg | tail 查看 dmesg 是否有系统层面错误日志 vmstat 1 检查 cpu 的负载情况, 是否存在 cpu 饱和 mpstat -P ALL 1 检查CPU是否存在负载不均衡; 单个过于忙碌的CPU可能意味着整个应用只有单个线程在工作 pidstat 1 观察随时间变化的 cpu、内存等信息 iostat -xz 1 弄清块设备（磁盘）的状况, 包括工作负载和处理性能 free -m 查看剩余内存, 文件和 IO 缓存大小 sar -n DEV 1 检查网络流量的工作负载：rxkB/s和txkB/s, 以及它是否达到限额了 sar -n TCP,ETCP 1 查看系统网络负载, 过多重传可能是服务器过载开始丢包了 top 没什么好多, 大家都会用的 top 各个指标含义 Cpu(s)：表示这一行显示CPU总体信息 0.0%us：用户态进程占用CPU时间百分比, 不包含renice值为负的任务占用的CPU的时间。 0.7%sy：内核占用CPU时间百分比 0.0%ni：改变过优先级的进程占用CPU的百分比 99.3%id：空闲CPU时间百分比 0.0%wa：等待I/O的CPU时间百分比 0.0%hi： CPU硬中断时间百分比 0.0%si： CPU软中断时间百分比 0.0%st: 超线程开启时候, 线程等待另外一个虚拟核完成任务的时间百分比 note：这里显示数据是所有cpu的平均值, 如果想看每一个cpu的处理情况, 按1即可；折叠, 再次按1 中断硬中断 硬中断是由硬件产生的，比如，像磁盘，网卡，键盘，时钟等。每个设备或设备集都有它自己的IRQ（中断请求）。基于IRQ，CPU可以将相应的请求分发到对应的硬件驱动上（注：硬件驱动通常是内核中的一个子程序，而不是一个独立的进程）。 处理中断的驱动是需要运行在CPU上的，因此，当中断产生的时候，CPU会中断当前正在运行的任务，来处理中断。在有多核心的系统上，一个中断通常只能中断一颗CPU（也有一种特殊的情况，就是在大型主机上是有硬件通道的，它可以在没有主CPU的支持下，可以同时处理多个中断。）。 硬中断可以直接中断CPU。它会引起内核中相关的代码被触发。对于那些需要花费一些时间去处理的进程，中断代码本身也可以被其他的硬中断中断。 对于时钟中断，内核调度代码会将当前正在运行的进程挂起，从而让其他的进程来运行。它的存在是为了让调度代码（或称为调度器）可以调度多任务。 软中断 软中断的处理非常像硬中断。然而，它们仅仅是由当前正在运行的进程所产生的。 通常，软中断是一些对I/O的请求。这些请求会调用内核中可以调度I/O发生的程序。对于某些设备，I/O请求需要被立即处理，而磁盘I/O请求通常可以排队并且可以稍后处理。根据I/O模型的不同，进程或许会被挂起直到I/O完成，此时内核调度器就会选择另一个进程去运行。I/O可以在进程之间产生并且调度过程通常和磁盘I/O的方式是相同。 软中断仅与内核相联系。而内核主要负责对需要运行的任何其他的进程进行调度。一些内核允许设备驱动的一些部分存在于用户空间，并且当需要的时候内核也会调度这个进程去运行。 软中断并不会直接中断CPU。也只有当前正在运行的代码（或进程）才会产生软中断。这种中断是一种需要内核为正在运行的进程去做一些事情（通常为I/O）的请求。有一个特殊的软中断是Yield调用，它的作用是请求内核调度器去查看是否有一些其他的进程可以运行。 释疑 对于软中断，I/O 操作是否是由内核中的 I/O 设备驱动程序完成？ 对于 I/O 请求，内核会将这项工作分派给合适的内核驱动程序，这个程序会对 I/O 进行队列化，以可以稍后处理（通常是磁盘I/O），或如果可能可以立即执行它。通常，当对硬中断进行回应的时候，这个队列会被驱动所处理。当一个 I/O 请求完成的时候，下一个在队列中的 I/O 请求就会发送到这个设备上。 软中断所经过的操作流程是比硬中断的少吗？换句话说，对于软中断就是：进程 -&gt; 内核中的设备驱动程序；对于硬中断：硬件 -&gt; CPU 内核中的设备驱动程序？ 是的，软中断比硬中断少了一个硬件发送信号的步骤。产生软中断的进程一定是当前正在运行的进程，因此它们不会中断CPU。但是它们会中断调用代码的流程。 如果硬件需要 CPU 去做一些事情，那么这个硬件会使 CPU 中断当前正在运行的代码。而后 CPU 会将当前正在运行进程的当前状态放到堆栈（stack）中，以至于之后可以返回继续运行。这种中断可以停止一个正在运行的进程；可以停止正处理另一个中断的内核代码；或者可以停止空闲进程。 查看 cpu 超线程和 cpu 物理核对应关系123456789101112131415161718192021222324252627282930~$ lscpuArchitecture: x86_64CPU op-mode(s): 32-bit, 64-bitByte Order: Little EndianCPU(s): 2On-line CPU(s) list: 0,1Thread(s) per core: 1Core(s) per socket: 2Socket(s): 1NUMA node(s): 1Vendor ID: GenuineIntelCPU family: 6Model: 61Model name: Intel(R) Core(TM) i5-5257U CPU @ 2.70GHzStepping: 4CPU MHz: 2699.998BogoMIPS: 5399.99Hypervisor vendor: KVMVirtualization type: fullL1d cache: 32KL1i cache: 32KL2 cache: 256KL3 cache: 3072KNUMA node0 CPU(s): 0,1~$ cat /sys/devices/system/node/node1/cpulist1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39~$ cat /sys/devices/system/node/node0/cpulist0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38 机器总共有 2 个物理核心，每个物理核心有 10 个逻辑核，每个逻辑核有 2 个超线程 查看进程和 cpu 亲缘性绑定关系1234567891011ps -eo pid,argssrpid - 进程IDargs - 该进程执行时传入的命令行参数psr - 分配给进程的逻辑CPU~$ ps -eo pid,argssr | grep nginx9073 nginx: master process /usr/ 19074 nginx: worker process 09075 nginx: worker process 19076 nginx: worker process 29077 nginx: worker process 3 查看中断号和 cpu 绑定关系1234567891011121314151617~$ cat /proc/interrupts CPU0 CPU1 162: 1051061576 0 IR-PCI-MSI-edge eth0-TxRx-0 163: 85025461 133849653 IR-PCI-MSI-edge eth0-TxRx-1 164: 88387458 0 IR-PCI-MSI-edge eth0-TxRx-2 165: 138497867 0 IR-PCI-MSI-edge eth0-TxRx-3 166: 187011870 0 IR-PCI-MSI-edge eth0-TxRx-4 167: 71803161 35003925 IR-PCI-MSI-edge eth0-TxRx-5 168: 65314317 0 IR-PCI-MSI-edge eth0-TxRx-6 169: 64069078 0 IR-PCI-MSI-edge eth0-TxRx-7 170: 138158880 0 IR-PCI-MSI-edge eth0-TxRx-8 171: 40424137 20524266 IR-PCI-MSI-edge eth0-TxRx-9 172: 134531725 0 IR-PCI-MSI-edge eth0-TxRx-10~$ sudo cat /proc/irq/165/smp_affinity00000000,00000000,00000000,00000000,00000000,00000008 首先查看系统所有中断号，查看多对列网卡 eth0 的中断号查看中断号分配的 cpu, 0x00000008 代表 cpu 8 others123ethtools -S eth0 查看网络的队列信息ethanol -i eth0 查看网卡信息lspci -vvv 查看pci 相关信息 iperf 网卡测试-P 指定一个并发, 如果指定多个并发，统计信息不友好12345server:iperf -sclient:iperf -c 10.200.136.41 -p 5001 -P 1 -t 60 -i 1 查看网卡情况netstat 查看 Errorsifconfig 查看 dropped 数量1234567891011121314151617181920~$ watch &apos;netstat -s --udp&apos;Udp: 437.0k/s packets received 0.0/s packets to unknown port received. 386.9k/s packet receive errors 0.0/s packets sent RcvbufErrors: 123.8k/s SndbufErrors: 0 InCsumErrors: 0~$ ifconfigenp0s8: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 192.168.56.101 netmask 255.255.255.0 broadcast 192.168.56.255 inet6 fe80::b5e6:56d6:48dd:e317 prefixlen 64 scopeid 0x20&lt;link&gt; ether 08:00:27:e6:c8:ac txqueuelen 1000 (Ethernet) RX packets 71 bytes 20634 (20.1 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 81 bytes 13593 (13.2 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux 命令助记]]></title>
    <url>%2F2018%2F09%2F24%2Flinux-command-note%2F</url>
    <content type="text"><![CDATA[print all system information1uname -a kernel version:1uname -r see hardware info.1cat /proc/meminfo cat /proc/cpuinfo count line1find . -name &apos;\*.c&apos; | xargs wc -l &#123;&#125;\; yum remove without dependency1sudo rpm -e --nodeps vim-common-7.4.160-1.el7.x86_64 fio test1sudo /usr/local/bin/fio --filename=/dev/sda --direct=1 --rw=randrw --refill_buffers --norandommap --randrepeat=0 --ioengine=libaio --bs=4k --rwmixread=100 --iodepth=16 --numjobs=16 --runtime=1000 --group_reporting --name=4ktest docker expose port to host machine1docker run -p 8080:3000 my-image docker get running container bash cmd1docker exec -it contained bash tcpdump1sudo tcpdump -i any port 9200 -w o.pcap 批量结束进程1kill -9 `ps -ef|grep nginx|grep -v grep |awk &apos;&#123;print $2&#125;&apos;` 查看进程占用端口123456ps -ef | grep processnamenetstat -nap | grep pidlsof -i:9090pwdx 9090lsof -P 9090 | grep cwd vim replace1: %s/\(randrecord\.get(&apos;ip&apos;)\)/ptest\.\1/g 将文件中的所有 randrecord.get(‘ip’) 替换为 test.randrecord.get(‘ip’)，即使用 \1 \2 等可以匹配前面第几个括号内的内容，此括号需要使用 反斜杠 转意 查看 glibc 版本1strings /lib64/libc.so.6 |grep GLIBC linux 将标出输出重定向到 /dev/null12&gt;&amp; 1 awk 打印奇偶行1awk &apos;&#123;print $0 &gt; NR%2&#125;&apos; file sort by frequence1cat tmp.txt| sort | uniq -c | sort -k1,1nr | head -30 &gt; stoplist.txt filter by stoplist1tr &apos; &apos; &apos;\n&apos; &lt; stoplist.txt | grep -vwFf - tmp.txt 字符串拼接1ps -ef | grep zicogo | awk &apos;BEGIN&#123;sum=&quot;&quot;&#125;&#123;sum=($2&quot;,&quot;sum)&#125;END&#123;print sum&#125;&apos; 按组统计求和1awk &apos;&#123;s[$1] += $2; a[$1] += $3 &#125;END&#123; for(i in s)&#123; printf &quot;%-50s %-20d %-20d\n&quot;, i,s[i],a[i] &#125; &#125;&apos; upstats-2017-07-05-15-05.log+ 查看某个库的版本号1ldconfig -p | grep libssl 删除 archlinux 软件包1pacman -Scc history clean1cat .zsh_history | awk &apos;BEGIN&#123;FS=&quot;&quot;&#125;&#123;if (NF &gt; 40 ) print ;&#125;&apos; &gt;&gt; .zsh_history_new brew install1brew install vim --with-lua --with-override-system-vi --build-from-source global floder sed1sed -i &quot;s/TINY/NOX/g&quot; `grep -R TINY -rl ./` 批量查找替换1find CHANGELOG.md -type f -exec vim +&quot;retab | wq&quot; &#123;&#125; \; 查看用户所属组1id -g -n $whoami 列出 osx 系统的 java 版本1/usr/libexec/java_home -V 列出 java 的信任证书1keytool -list -keystore /Library/Java/JavaVirtualMachines/jdk1.8.0_171.jdk/Contents/Home/jre/lib/security/cacerts java 导入证书1keytool -import -trustcacerts -file [certificate] -alias [alias] -keystore $JAVA_HOME/lib/security/cacerts vim highlight1:so $VIMRUNTIME/syntax/hitest.vim check cpu info123456789sudo dmidecode -t 4 | grep -E &apos;Socket Designation|Count&apos; Socket Designation: CPU1 Core Count: 8 Thread Count: 16 Socket Designation: CPU2 Core Count: 8 Thread Count: 16lscpu awk 和 grep 日志统计分析12cat nginx/logs/access.log | grep &quot;17/Sep&quot; | grep -v &apos;2\.43&apos; | awk &apos;&#123;if ($11 &gt; 1) print $0&#125;&apos;cat nginx/logs/access.log | grep &quot;17/Sep&quot; | grep -v &apos;2\.43&apos; | awk &apos;BEGIN&#123;a=0&#125;&#123;if ($11&gt;0+a) a=$11&#125; END&#123;print a&#125;&apos; centos 查看某个库是哪个 package 提供的1yum whatprovides libpci.so.3 查看 PPID1top -&gt; G -&gt; 2]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[updns QPS 异常问题排查]]></title>
    <url>%2F2018%2F09%2F24%2Fupdns-qps-tracing%2F</url>
    <content type="text"><![CDATA[环境问题排查 系统: 系统版本, 内核版本 cpu: 物理核心数, 物理核数,逻辑核数 内存: 内存大小 网卡: 网卡是否是万兆, 驱动型号 updns 版本确认（排查是否最近代码导致） 以前压测过的版本回退 v0.03 使用最近版本测试 系统各种参数优化确认 sendmmsg recvmmsg 多线程发包 so_reuseport 绑定网卡亲缘性 系统参数调优化 查看软件终端, 查看 cpu 绑定是否生效 使用系统工具确认压测机器是否正常 iperf 网络ok, 吞吐量OK, QPS: 60s 400W QPS, 800Mb/s udp sender 查看发包 80w/s, 系统占用低 perf spin_irq_lock 60% 查看网卡的收包, 接包, 丢包情况 被测端和压测端的情况]]></content>
  </entry>
  <entry>
    <title><![CDATA[nginx server locaton 匹配顺序]]></title>
    <url>%2F2018%2F09%2F24%2Fnginx-server-locaton-re-match%2F</url>
    <content type="text"><![CDATA[server name 完全匹配 通配符在前面，*.tesetwb.com 通配符在后面，www.testwb.* 正则表达式匹配 (.*)?anyfeel.cn location 完全匹配，= or / 大小写敏感，~ 大小写不敏感，~* 前半部分匹配，^~ @location, 表示只能用于 nginx 内部跳转]]></content>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx 模块初始化过程]]></title>
    <url>%2F2018%2F09%2F24%2Fnginx-module-init%2F</url>
    <content type="text"><![CDATA[初始化核心函数ngx_init_cycle()(ngx_cycle.c:275) -&gt; ngx_conf_parse()(ngx_conf_file.c:) -&gt; ngx_conf_handler() 初始化步骤 modules 和序号绑定关系123456789101112131415ngx_int_tngx_preinit_modules(void)&#123; ngx_uint_t i; for (i = 0; ngx_modules[i]; i++) &#123; ngx_modules[i]-&gt;index = i; ngx_modules[i]-&gt;name = ngx_module_names[i]; &#125; ngx_modules_n = i; ngx_max_module = ngx_modules_n + NGX_MAX_DYNAMIC_MODULES; return NGX_OK;&#125; ngx_init_cycle 初始化核心模块 12345678910111213141516171819202122232425262728293031323334353637383940...for (i = 0; cycle-&gt;modules[i]; i++) &#123; if (cycle-&gt;modules[i]-&gt;type != NGX_CORE_MODULE) &#123; continue; &#125; module = cycle-&gt;modules[i]-&gt;ctx; if (module-&gt;create_conf) &#123; rv = module-&gt;create_conf(cycle); if (rv == NULL) &#123; ngx_destroy_pool(pool); return NULL; &#125; cycle-&gt;conf_ctx[cycle-&gt;modules[i]-&gt;index] = rv; &#125;&#125;...for (i = 0; cycle-&gt;modules[i]; i++) &#123; if (cycle-&gt;modules[i]-&gt;type != NGX_CORE_MODULE) &#123; // NGX_CORE_MODULE 核心模块 continue; &#125; module = cycle-&gt;modules[i]-&gt;ctx; if (module-&gt;init_conf) &#123; if (module-&gt;init_conf(cycle, cycle-&gt;conf_ctx[cycle-&gt;modules[i]-&gt;index]) // 初始化 == NGX_CONF_ERROR) &#123; environ = senv; ngx_destroy_cycle_pools(&amp;conf); return NULL; &#125; &#125;&#125;... 调用 ngx_events_block 来解析具体模块如配置和指令, 以 event 模块为例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354static char *ngx_events_block(ngx_conf_t *cf, ngx_command_t *cmd, void *conf)&#123; ... *(void **) conf = ctx; for (i = 0; cf-&gt;cycle-&gt;modules[i]; i++) &#123; if (cf-&gt;cycle-&gt;modules[i]-&gt;type != NGX_EVENT_MODULE) &#123; continue; &#125; m = cf-&gt;cycle-&gt;modules[i]-&gt;ctx; if (m-&gt;create_conf) &#123; (*ctx)[cf-&gt;cycle-&gt;modules[i]-&gt;ctx_index] = m-&gt;create_conf(cf-&gt;cycle); // 为每个 event 模块分配内存 if ((*ctx)[cf-&gt;cycle-&gt;modules[i]-&gt;ctx_index] == NULL) &#123; return NGX_CONF_ERROR; &#125; &#125; &#125; pcf = *cf; cf-&gt;ctx = ctx; cf-&gt;module_type = NGX_EVENT_MODULE; cf-&gt;cmd_type = NGX_EVENT_CONF; rv = ngx_conf_parse(cf, NULL); // 解析 event 模块，如 `work_connections` *cf = pcf; if (rv != NGX_CONF_OK) &#123; return rv; &#125; for (i = 0; cf-&gt;cycle-&gt;modules[i]; i++) &#123; if (cf-&gt;cycle-&gt;modules[i]-&gt;type != NGX_EVENT_MODULE) &#123; continue; &#125; m = cf-&gt;cycle-&gt;modules[i]-&gt;ctx; if (m-&gt;init_conf) &#123; rv = m-&gt;init_conf(cf-&gt;cycle, (*ctx)[cf-&gt;cycle-&gt;modules[i]-&gt;ctx_index]); // 和指令默认值对比，设置指令的值 if (rv != NGX_CONF_OK) &#123; return rv; &#125; &#125; &#125; return NGX_CONF_OK;&#125;]]></content>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP 连接状态整理]]></title>
    <url>%2F2018%2F09%2F24%2Ftcp-connection-status%2F</url>
    <content type="text"><![CDATA[要点 五层架构与七层架构 链路层作用 ARP，以太网包格式 IP/TCP 格式 TCP 状态机及常用的错误码，time_wait 相关系，SO_RESUSEADDR 作用。 NAGEL 算法(TCP_NODELAY关闭NAGEL)滑动窗口拥塞避免（慢启动），快速重传与快速恢复，选择重传 四种定时器的作用 ACK 定时器，persist 定时器，keep-alive 定时器，2MSL 定时器 使用 TCPDUMP 和应用程序分析 TCP 状态机 note:so_linger 作用： 发送 RST 包来断开连接，而不是发送 FIN. connect resty by peer含义本端向对端发送数据，但对端无法识别该连接，返回一个 RST 强制关闭连接 原因 当尝试和未开放的服务器端口建立tcp连接时，服务器tcp将会直接向客户端发送reset报文，表现状态是连接决绝。 双方之前已经正常建立了通信通道，也可能进行过了交互，当某一方在交互的过程中发生了异常，如崩溃等，异常的一方会向对端发送reset报文，通知对方将连接关闭 当收到TCP报文，但是发现该报文不是已建立的TCP连接列表可处理的，则其直接向对端发送reset报文 ack报文丢失，并且超出一定的重传次数或时间后，会主动向对端发送reset报文释放该TCP连接 一端设置了 SO_LINGER 选项来关闭连接，则对端会收到 connection by peer 错误来关闭连接 TCP 状态机]]></content>
      <tags>
        <tag>TCP/IP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 磁盘调整]]></title>
    <url>%2F2018%2F09%2F23%2Fadjust-disk-partition-size%2F</url>
    <content type="text"><![CDATA[磁盘工具 parted/fdisk/gdisk/ blkid note: 注意区分 PARTUUID 和 UUID gdisk sort 可以调整分区序号 磁盘格式化 EFI 分区格式化1mkfs.fat -F32 /dev/sdxY -&gt; EFI 分区格式化 如果没有 mkfs.fat 则安装 dosfstools 分区格式化1mkfs.ext4 /dev/sda1 磁盘启动管理 安装 bootctl 1bootctl —path=/mnt/boot install 增加磁盘启动选项 1/boot/loader/entries/arch.conf 这里写入的是 root 分区的 PARTUUID 磁盘分区大小调整(resize) 先删除原有分区，然后再重新建立（启动区块号不能变，否则会丢失数据） 使用 resize2fs /dev/sda1 来重新调整分区大小 最后需要解决 EFI loader 中 PARTUUID，PARTUUID 可能会有变更]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 pyenv 构建线上 python 环境]]></title>
    <url>%2F2018%2F09%2F23%2Fdeploy-python-program-by-pyenv%2F</url>
    <content type="text"><![CDATA[使用 pyenv 部署 python 线上环境当生产服务器因权限问题或者系统版本阉割出现 python 依赖问题时，可以使用 pyenv 安装方法 下载 pyenv. 1$ git clone https://github.com/yyuu/pyenv.git ~/.pyenv 设置PYENV_ROOT 为 pyenv 安装路径, 添加 $PYENV_ROOT/bin 至环境变量 12$ echo &apos;export PYENV_ROOT=&quot;$HOME/.pyenv&quot;&apos; &gt;&gt; ~/.bash_profile$ echo &apos;export PATH=&quot;$PYENV_ROOT/bin:$PATH&quot;&apos; &gt;&gt; ~/.bash_profile 开启 pyenv shims 和自动补全 1$ echo &apos;eval &quot;$(pyenv init -)&quot;&apos; &gt;&gt; ~/.bash_profile 重启 shell 让 pyenv 生效 1$ exec $SHELL 安装需要的 python 版本 至 $PYENV_ROOT/versions 目录 1$ pyenv install 2.7.8 pyenv 常用命令12345pyenv install 2.7.5pyenv versionspyenv versionpyenv local 2.7.5pyenv shell 2.7.5]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[run nsq by docker]]></title>
    <url>%2F2018%2F09%2F23%2Frun-nsq-by-docker%2F</url>
    <content type="text"><![CDATA[componentsnsqdnsqd is the daemon that receives, queues, and delivers messages to clients.two tcp ports: one for clients and one for HTTP API nsqlookupdthe daemon that manages topology information.Clients query nsqlookupd to discover nsqd producers for a specific topic and nsqd nodes broadcasts topic and channel information.two interfaces: A TCP interface which is used by nsqd for broadcasts and an HTTP interface for clients to perform discovery and administrative actions. nsqadminWeb UI to view aggregated cluster stats in realtime and perform various administrative tasks run nsqget nsq docker image1docker pull nsqio/nsq run nsqlookupd1docker run --name lookupd -d -p 4160:4160 -p 4161:4161 nsqio/nsq /nsqlookupd run nsqd get docker host’s ip 1ifconfig | grep docker addr -&gt; 172.17.0.1 run nsqd container 12docker run --name nsqd -d -p 4150:4150 -p 4151:4151 nsqio/nsq /nsqd --broadcast-address=&lt;host&gt; --lookupd-tcp-address=&lt;host&gt;:&lt;port&gt; eg:12docker run --name nsqd -d -p 4150:4150 -p 4151:4151 nsqio/nsq /nsqd --broadcast-address=172.17.0.1 --lookupd-tcp-address=172.17.0.1:4160 note:a. no care for 4150 and 4151b. –broadcast-address set to the docker addrc. –lookupd-tcp-address set to the nsqlookupd http listen port, here is 4161 run nsqadmin1docker run -d --name nsqadmin -p 4171:4171 nsqio/nsq /nsqadmin --lookupd-http-address=172.17.0.1:4161]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 编码理解]]></title>
    <url>%2F2018%2F09%2F23%2FLinux-encode-thinking%2F</url>
    <content type="text"><![CDATA[系统的编码方式在我所使用的 centos6 中使用 echo $LANG 查看编码系统的编码方式决定了在终端中录入的内容的编码方式 编辑器所使用的文件编码方式在 vim 中，set fileencoding 查看当前编码方式编辑器的编码方式决定了在代码中录入内容的编码方式，如 a[‘key’] = value，此时的 ‘key’ 字段按照 fileencoding 的方式编码 代码中使用的编码方式例如 if a == b : pass，此内容的编码方式 从文件中读取的内容的编码方式例如：12redis_cli&gt;&gt; set “marco:domains” “basic.b0.upaiyun.com” “basic&quot; 此写入redis的内容，根据终端的编码方式决定， 我的系统默认是utf-8 ，所以此处也是utf-8内容 解释器的编码方式python 解释器使用 unicode 编码方式解释执行 note:在我使用的环境中为 centos + vim 开发环境，处处是 utf-8 编码 python 编码中文处理 总体思路：程序整体内部使用 python 解释器的内建 ascii, 在文件落地本地存储的时候再转存为 utf-8 在处理中文字符串的时候，由于系统编码的差异直接使用 string 类型表示中文的时候错误，使用 unicode 表示。 使用 str.decode(‘utf-8’)转换为 unicode object]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[netcat skills]]></title>
    <url>%2F2018%2F09%2F23%2Fnetcat-skills%2F</url>
    <content type="text"><![CDATA[Linux netcat 命令实例：端口扫描端口扫描经常被系统管理员和黑客用来发现在一些机器上开放的端口，帮助他们识别系统中的漏洞。 $nc -z -v -n 172.31.100.7 21-25 可以运行在TCP或者UDP模式，默认是TCP，-u参数调整为udp. z 参数告诉netcat使用0 IO,连接成功后立即关闭连接， 不进行数据交换 v 参数指使用冗余选项 n 参数告诉netcat 不要使用DNS反向查询IP地址的域名 这个命令会打印21到25 所有开放的端口。Banner是一个文本，Banner是一个你连接的服务发送给你的文本信息。当你试图鉴别漏洞或者服务的类型和版本的时候，Banner信息是非常有用的。但是，并不是所有的服务都会发送banner。 一旦你发现开放的端口，你可以容易的使用netcat 连接服务抓取他们的banner。 $ nc -v 172.31.100.7 21 netcat 命令会连接开放端口21并且打印运行在这个端口上服务的banner信息。 Chat Server假如你想和你的朋友聊聊，有很多的软件和信息服务可以供你使用。但是，如果你没有这么奢侈的配置，比如你在计算机实验室，所有的对外的连接都是被限制的，你怎样和整天坐在隔壁房间的朋友沟通那？不要郁闷了，netcat提供了这样一种方法，你只需要创建一个Chat服务器，一个预先确定好的端口，这样子他就可以联系到你了。 Server$nc -l 1567netcat 命令在1567端口启动了一个tcp 服务器，所有的标准输出和输入会输出到该端口。输出和输入都在此shell中展示。 Client$nc 172.31.100.7 1567不管你在机器B上键入什么都会出现在机器A上。 文件传输大部分时间中，我们都在试图通过网络或者其他工具传输文件。有很多种方法，比如FTP,SCP,SMB等等，但是当你只是需要临时或者一次传输文件，真的值得浪费时间来安装配置一个软件到你的机器上嘛。假设，你想要传一个文件file.txt 从A 到B。A或者B都可以作为服务器或者客户端，以下，让A作为服务器，B为客户端。 Server$nc -l 1567 &lt; file.txtClient$nc -n 172.31.100.7 1567 &gt; file.txt这里我们创建了一个服务器在A上并且重定向netcat的输入为文件file.txt，那么当任何成功连接到该端口，netcat会发送file的文件内容。 在客户端我们重定向输出到file.txt，当B连接到A，A发送文件内容，B保存文件内容到file.txt. 没有必要创建文件源作为Server，我们也可以相反的方法使用。像下面的我们发送文件从B到A，但是服务器创建在A上，这次我们仅需要重定向netcat的输出并且重定向B的输入文件。 B作为Server Server$nc -l 1567 &gt; file.txtClientnc 172.31.100.23 1567 &lt; file.txt 目录传输发送一个文件很简单，但是如果我们想要发送多个文件，或者整个目录，一样很简单，只需要使用压缩工具tar，压缩后发送压缩包。 如果你想要通过网络传输一个目录从A到B。 Server$tar -cvf – dir_name | nc -l 1567Client$nc -n 172.31.100.7 1567 | tar -xvf -这里在A服务器上，我们创建一个tar归档包并且通过-在控制台重定向它，然后使用管道，重定向给netcat，netcat可以通过网络发送它。 在客户端我们下载该压缩包通过netcat 管道然后打开文件。 如果想要节省带宽传输压缩包，我们可以使用bzip2或者其他工具压缩。 Server$tar -cvf – dir_name| bzip2 -z | nc -l 1567通过bzip2压缩 Client$nc -n 172.31.100.7 1567 | bzip2 -d |tar -xvf -使用bzip2解压 加密你通过网络发送的数据如果你担心你在网络上发送数据的安全，你可以在发送你的数据之前用如mcrypt的工具加密。 服务端$nc localhost 1567 | mcrypt –flush –bare -F -q -d -m ecb &gt; file.txt使用mcrypt工具加密数据。 客户端$mcrypt –flush –bare -F -q -m ecb &lt; file.txt | nc -l 1567使用mcrypt工具解密数据。 以上两个命令会提示需要密码，确保两端使用相同的密码。 这里我们是使用mcrypt用来加密，使用其它任意加密工具都可以。 流视频虽然不是生成流视频的最好方法，但如果服务器上没有特定的工具，使用netcat，我们仍然有希望做成这件事。 服务端$cat video.avi | nc -l 1567这里我们只是从一个视频文件中读入并重定向输出到netcat客户端$nc 172.31.100.7 1567 | mplayer -vo x11 -cache 3000 -这里我们从socket中读入数据并重定向到mplayer。 克隆一个设备如果你已经安装配置一台Linux机器并且需要重复同样的操作对其他的机器，而你不想在重复配置一遍。不在需要重复配置安装的过程，只启动另一台机器的一些引导可以随身碟和克隆你的机器。 克隆Linux PC很简单，假如你的系统在磁盘/dev/sda上 Server$dd if=/dev/sda | nc -l 1567Client$nc -n 172.31.100.7 1567 | dd of=/dev/sdadd是一个从磁盘读取原始数据的工具，我通过netcat服务器重定向它的输出流到其他机器并且写入到磁盘中，它会随着分区表拷贝所有的信息。但是如果我们已经做过分区并且只需要克隆root分区，我们可以根据我们系统root分区的位置，更改sda 为sda1，sda2.等等。 打开一个shell我们已经用过远程shell-使用telnet和ssh，但是如果这两个命令没有安装并且我们没有权限安装他们，我们也可以使用netcat创建远程shell。 假设你的netcat支持 -c -e 参数(默认 netcat) Server$nc -l 1567 -e /bin/bash -iClient$nc 172.31.100.7 1567这里我们已经创建了一个netcat服务器并且表示当它连接成功时执行/bin/bash 假如netcat 不支持-c 或者 -e 参数（openbsd netcat）,我们仍然能够创建远程shell Server``$mkfifo /tmp/tmp_fifo $cat /tmp/tmp_fifo | /bin/sh -i 2&gt;1 | nc -l 1567 &gt; /tmp/tmp_fifo``这里我们创建了一个fifo文件，然后使用管道命令把这个fifo文件内容定向到shell 2>&amp;1中。是用来重定向标准错误输出和标准输出，然后管道到netcat 运行的端口1567上。至此，我们已经把netcat的输出重定向到fifo文件中。 说明： 从网络收到的输入写到fifo文件中 cat 命令读取fifo文件并且其内容发送给sh命令 sh命令进程受到输入并把它写回到netcat。 netcat 通过网络发送输出到client 至于为什么会成功是因为管道使命令平行执行，fifo文件用来替代正常文件，因为fifo使读取等待而如果是一个普通文件，cat命令会尽快结束并开始读取空文件。 在客户端仅仅简单连接到服务器 Client$nc -n 172.31.100.7 1567你会得到一个shell提示符在客户端 反向shell 反向shell是指在客户端打开的shell。反向shell这样命名是因为不同于其他配置，这里服务器使用的是由客户提供的服务。 服务端$nc -l 1567在客户端，简单地告诉netcat在连接完成后，执行shell。 客户端$nc 172.31.100.7 1567 -e /bin/bash现在，什么是反向shell的特别之处呢 反向shell经常被用来绕过防火墙的限制，如阻止入站连接。例如，我有一个专用IP地址为172.31.100.7，我使用代理服务器连接到外部网络。如果我想从网络外部访问 这台机器如1.2.3.4的shell，那么我会用反向外壳用于这一目的。 指定源端口假设你的防火墙过滤除25端口外其它所有端口，你需要使用-p选项指定源端口。 服务器端$nc -l 1567客户端$nc 172.31.100.7 1567 -p 25使用1024以内的端口需要root权限。 该命令将在客户端开启25端口用于通讯，否则将使用随机端口。]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[build bind with edns support]]></title>
    <url>%2F2018%2F09%2F23%2Fbuild-bind-with-edns-support%2F</url>
    <content type="text"><![CDATA[download and extract BIND.123$ wget ftp://ftp.isc.org/isc/bind9/9.9.3/bind-9.9.3.tar.gz$ tar xf bind-9.9.3.tar.gz$ cd bind-9.9.3 Download the patch from Wilmer van der Gaast.1$ wget http://wilmer.gaa.st/edns-client-subnet/bind-9.9.3-dig-edns-client-subnet-iana.diff Patch the code, configure (without OpenSSL because we only want dig) and compile.123$ patch -p0 &lt; bind-9.9.3-dig-edns-client-subnet-iana.diff$ ./configure --without-openssl$ make Now you will have dig placed in bin/dig. You can try it this way:1$ ./bin/dig/dig @ns1.google.com www.google.es +client=157.88.0.0/16 Note the CLIENT-SUBNET line in the answer OPT PSEUDOSECTION.&gt;]]></content>
      <tags>
        <tag>DNS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Unix 环境变量加载顺序]]></title>
    <url>%2F2018%2F09%2F23%2Funix-env-load-order%2F</url>
    <content type="text"><![CDATA[OSXMac系统的环境变量加载顺序为 1/etc/profile -&gt; /etc/paths -&gt; ~/.bash_profile -&gt; ~/.bash_login -&gt; ~/.profile -&gt; ~/.bashrc 特别注意 /etc/paths 中的内容12345/usr/bin/bin/usr/sbin/sbin/usr/local/bin Homebrew 安装的软件，其二进制执行文件都放在/usr/local/bin中，bin 在使用时的查找不是覆盖原则，而是优先查找，所以例如 mac 已经自带了sqlite3，如果 brew 安装后，最新版的 sqlite3 是不会被调用的，因此可以将顺序修改一下以达到目的。 LinuxMac系统的环境变量加载顺序为1/etc/profile -&gt; (~/.bash_profile | ~/.bash_login | ~/.profile) -&gt; ~/.bashrc -&gt; /etc/bashrc -&gt; ~/.bash_logout /usr/bin:usr/sbin 在 /etc/profile 文件中]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx 基于权重的平滑轮询算法]]></title>
    <url>%2F2018%2F03%2F19%2Fload-balance-algorithm%2F</url>
    <content type="text"><![CDATA[Nginx 基于权重的轮询算法Nginx基于权重的轮询算法的实现可以参考它的一次代码提交 Upstream: smooth weighted round-robin balancing 它不但实现了基于权重的轮询算法，而且还实现了平滑的算法。所谓平滑，就是在一段时间内，不仅服务器被选择的次数的分布和它们的权重一致，而且调度算法还比较均匀的选择服务器，而不会集中一段时间之内只选择某一个权重比较高的服务器。如果使用随机算法选择或者普通的基于权重的轮询算法，就比较容易造成某个服务集中被调用压力过大。 举个例子，比如权重为 {a:5, b:1, c:1} 的一组服务器，Nginx 的平滑的轮询算法选择的序列为{ a, a, b, a, c, a, a },这显然要比{ c, b, a, a, a, a, a } 序列更平滑，更合理，不会造成对a服务器的集中访问。 Lua 实现每次需要遍历所有的 servers 列表，返回 best server 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869local ceil = math.ceillocal _M = &#123; _VERSION = &quot;0.11&quot; &#125;--[[parameters: - (table) servers - (function) peer_cb(index, server)return: - (table) server - (string) error--]]function _M.next_round_robin_server(servers, peer_cb) local srvs_cnt = #servers if srvs_cnt == 1 then if peer_cb(1, servers[1]) then return servers[1], nil end return nil, &quot;round robin: no servers available&quot; end -- select round robin server local best local max_weight local weight_sum = 0 for idx = 1, srvs_cnt do local srv = servers[idx] -- init round robin state srv.weight = srv.weight or 1 srv.effective_weight = srv.effective_weight or srv.weight srv.current_weight = srv.current_weight or 0 if peer_cb(idx, srv) then srv.current_weight = srv.current_weight + srv.effective_weight weight_sum = weight_sum + srv.effective_weight if srv.effective_weight &lt; srv.weight then srv.effective_weight = srv.effective_weight + 1 end if not max_weight or srv.current_weight &gt; max_weight then max_weight = srv.current_weight best = srv end end end if not best then return nil, &quot;round robin: no servers available&quot; end best.current_weight = best.current_weight - weight_sum return best, nilendfunction _M.free_round_robin_server(srv, failed) if not failed then return end srv.effective_weight = ceil((srv.effective_weight or 1) / 2)endreturn _M]]></content>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Unix tcpdump 使用技巧]]></title>
    <url>%2F2018%2F03%2F18%2Ftcpdump-using-skill%2F</url>
    <content type="text"><![CDATA[常用 tcpdump 参数解析 -i 指定网卡，一般不清楚网卡设置直接使用 “any” 表示抓取所有网卡 -A 使用 ASCII 码打印收到的每个包 -X 同时以十六进制和 ASCII 打印包 常用抓包实例1234567891011# host 和 port 过滤tcpdump -i any -Ans 0 &quot;src host 1.1.1.1 &amp;&amp; dst host 2.2.2.2 &amp;&amp; dst port 3100&quot;# GETtcpdump -i eth1 &apos;tcp[(tcp[12]&gt;&gt;2):4] = 0x47455420&apos;# POSTtcpdump -i eth1 &apos;tcp[(tcp[12]&gt;&gt;2):4] = 0x504f5354&apos;# TCP 标志位tcpdump -i any -Ans 0 &apos;host 183.214.154.4 &amp;&amp; tcp[tcpflags]=tcp-syn&apos; tcpdump 过滤器目标语法 dst 和 src 表示来源和目的地eg: src host and dst port 逻辑语法 and、or and noteg: src host1 or src host2 包内容过滤proto[x:y] 过滤从x字节开始的y字节数。比如ip[2:2]过滤出3、4字节（第一字节从0开始排 eg:1234tcp[(tcp[12]&gt;&gt;2):4] = 0x47455420tcp[12]&gt;&gt;4&lt;&lt;2 == tcp[12]&gt;&gt;22tcp[12]&gt;&gt;4 拿到 13 字节的 高 4 位，&gt;&gt;2相当于除以 4(32/8)，即偏移的 8 bits 位数 标志位过滤tcp[tcpflags]=tcp-syn TCP Header Format12345678910111213141516171819202122232425 0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Source Port | Destination Port | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Sequence Number | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Acknowledgment Number | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Data | |U|A|P|R|S|F| | | Offset| Reserved |R|C|S|S|Y|I| Window | | | |G|K|H|T|N|N| | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Checksum | Urgent Pointer | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Options | Padding | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | data | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+Data Offset: 4 bits The number of ```32 bit words``` in the TCP Header. This indicates where the data begins. The TCP header (even one including options) is an integral number of 32 bits long. IP format123456789101112131415 0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+|Version| IHL |Type of Service| Total Length |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+| Identification |Flags| Fragment Offset |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+| Time to Live | Protocol | Header Checksum |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+| Source Address |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+| Destination Address |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+| Options | Padding |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 pprof 和 Flame-Graph 调试 Golang 应用]]></title>
    <url>%2F2018%2F01%2F21%2Fusing-pprof-and-Flame-Graph-debug-Golang-Application%2F</url>
    <content type="text"><![CDATA[前言最近用 Golang 实现了一个日志搜集上报程序(内部称 logger 项目)，线上灰度测试过程发现 logger 占用 CPU 非常高(80% - 100%)。而此项目之前就在线上使用，用于消费 NSQ 任务， CPU 占用一直在 1%，最近的修改只是添加了基于磁盘队列的生产者消费者服务，生产者使用 go-gin 实现了一个 httpserver，接收数据后写入磁盘队列；消费者为单个 goroutine 循环 POST 数据。而 httpserver 压力不大(小于 100 QPS)，不至于占用如此高的 CPU，大致 review 代码后未发现异常，借助 pprof 和 flame-graph 来分析定位问题。 pprofpprof 我理解是 program profile(即程序性能剖析之意)，Golang 提供的两个官方包runtime/pprof，net/http/pprof能方便的采集程序运行的堆栈、goroutine、内存分配和占用、io 等信息的 .prof 文件，然后可以使用 go tool pprof 分析 .prof 文件。两个包的作用是一样的，只是使用方式的差异。 runtime/pprof如果程序为非 httpserver 类型，使用此方式；在 main 函数中嵌入如下代码: 1234567891011121314151617181920212223242526272829303132import &quot;runtime/pprof&quot;var cpuprofile = flag.String(&quot;cpuprofile&quot;， &quot;&quot;， &quot;write cpu profile `file`&quot;)var memprofile = flag.String(&quot;memprofile&quot;， &quot;&quot;， &quot;write memory profile to `file`&quot;)func main() &#123; flag.Parse() if *cpuprofile != &quot;&quot; &#123; f， err := os.Create(*cpuprofile) if err != nil &#123; log.Fatal(&quot;could not create CPU profile: &quot;， err) &#125; if err := pprof.StartCPUProfile(f); err != nil &#123; log.Fatal(&quot;could not start CPU profile: &quot;， err) &#125; defer pprof.StopCPUProfile() &#125; // ... rest of the program ... if *memprofile != &quot;&quot; &#123; f， err := os.Create(*memprofile) if err != nil &#123; log.Fatal(&quot;could not create memory profile: &quot;， err) &#125; runtime.GC() // get up-to-date statistics if err := pprof.WriteHeapProfile(f); err != nil &#123; log.Fatal(&quot;could not write memory profile: &quot;， err) &#125; f.Close() &#125;&#125; 运行程序 1./logger -cpuprofile cpu.prof -memprofile mem.prof 可以得到 cpu.prof 和 mem.prof 文件，使用 go tool pprof 分析。 12go tool pprof logger cpu.profgo tool pprof logger mem.prof net/http/pprof如果程序为 httpserver 类型， 则只需要导入该包: 1import _ &quot;net/http/pprof&quot; 如果 httpserver 使用 go-gin 包，而不是使用默认的 http 包启动，则需要手动添加 /debug/pprof 对应的 handler，github 有封装好的模版: 12345import &quot;github.com/DeanThompson/ginpprof&quot;...router := gin.Default()ginpprof.Wrap(router)... 导入包重新编译程序后运行，在浏览器中访问 http://host:port/debug/pprof 可以看到如下信息，这里 host 和 port 是程序绑定的 host 和 port，例如我自己的 logger 程序，访问如下地址: http://127.0.0.1:4500/debug/pprof/ 12345678910/debug/pprof/profiles:0 block62 goroutine427 heap0 mutex12 threadcreatefull goroutine stack dump 点击对应的 profile 可以查看具体信息，通过浏览器查看的数据不能直观反映程序性能问题，go tool pprof 命令行工具提供了丰富的工具集: 查看 heap 信息 1go tool pprof http://127.0.0.1:4500/debug/pprof/heap 查看 30s 的 CPU 采样信息 1go tool pprof http://127.0.0.1:4500/debug/pprof/profile 其他功能使用参见官方 net/http/pprof 库 pprof CPU 分析采集 profile 数据之后，可以分析 CPU 热点代码。执行下面命令： 1go tool pprof http://127.0.0.1:4500/debug/pprof/profile 会采集 30s 的 profile 数据，之后进入终端交互模式，输入 top 指令。 1234567891011121314151617181920212223~ # go tool pprof http://127.0.0.1:4500/debug/pprof/profileFetching profile over HTTP from http://127.0.0.1:4500/debug/pprof/profileSaved profile in /home/vagrant/pprof/pprof.logger.samples.cpu.012.pb.gzFile: loggerType: cpuTime: Jan 19， 2018 at 2:01pm (CST)Duration: 30s， Total samples = 390ms ( 1.30%)Entering interactive mode (type &quot;help&quot; for commands， &quot;o&quot; for options)(pprof) topShowing nodes accounting for 360ms， 92.31% of 390ms totalShowing top 10 nodes out of 74 flat flat% sum% cum cum% 120ms 30.77% 30.77% 180ms 46.15% compress/flate.(*compressor).findMatch /usr/local/go/src/compress/flate/deflate.go 100ms 25.64% 56.41% 310ms 79.49% compress/flate.(*compressor).deflate /usr/local/go/src/compress/flate/deflate.go 60ms 15.38% 71.79% 60ms 15.38% compress/flate.matchLen /usr/local/go/src/compress/flate/deflate.go 20ms 5.13% 76.92% 20ms 5.13% compress/flate.(*huffmanBitWriter).indexTokens /usr/local/go/src/compress/flate/huffman_bit_writer.go 10ms 2.56% 79.49% 10ms 2.56% compress/flate.(*huffmanBitWriter).writeTokens /usr/local/go/src/compress/flate/huffman_bit_writer.go 10ms 2.56% 82.05% 10ms 2.56% hash/adler32.update /usr/local/go/src/hash/adler32/adler32.go 10ms 2.56% 84.62% 10ms 2.56% runtime.futex /usr/local/go/src/runtime/sys_linux_amd64.s 10ms 2.56% 87.18% 10ms 2.56% runtime.memclrNoHeapPointers /usr/local/go/src/runtime/memclr_amd64.s 10ms 2.56% 89.74% 10ms 2.56% runtime.pcvalue /usr/local/go/src/runtime/symtab.go 10ms 2.56% 92.31% 10ms 2.56% runtime.runqput /usr/local/go/src/runtime/runtime2.go(pprof) 从统计可以 top5 操作全是数据压缩操作， logger 程序本身开启了压缩等级为 9 的 gzip 压缩，如果希望减少压缩 CPU 占用，可以调整压缩等级。 pprof mem 分析同时 pprof 也支持内存相关数据分析 --inuse_space 分析常驻内存1go tool pprof -alloc_space http://127.0.0.1:4500/debug/pprof/heap 1234567891011121314151617181920212223~ # go tool pprof -alloc_space http://127.0.0.1:4500/debug/pprof/heapFetching profile over HTTP from http://127.0.0.1:4500/debug/pprof/heapSaved profile in /home/vagrant/pprof/pprof.logger.alloc_objects.alloc_space.inuse_objects.inuse_space.006.pb.gzFile: loggerType: alloc_spaceTime: Jan 19， 2018 at 2:21pm (CST)Entering interactive mode (type &quot;help&quot; for commands， &quot;o&quot; for options)(pprof) topShowing nodes accounting for 47204.90MB， 99.16% of 47606.01MB totalDropped 230 nodes (cum &lt;= 238.03MB)Showing top 10 nodes out of 39 flat flat% sum% cum cum%28290.79MB 59.43% 59.43% 28290.79MB 59.43% bytes.makeSlice /usr/local/go/src/bytes/buffer.go 8706.78MB 18.29% 77.72% 10082.12MB 21.18% compress/flate.NewWriter /usr/local/go/src/compress/flate/deflate.go 8559.74MB 17.98% 95.70% 8559.74MB 17.98% github.com/nsqio/go-diskqueue.(*diskQueue).readOne /home/vagrant/go/src/github.com/nsqio/go-diskqueue/diskqueue.go 1343.78MB 2.82% 98.52% 1343.78MB 2.82% compress/flate.(*compressor).init /usr/local/go/src/compress/flate/deflate.go 298.81MB 0.63% 99.15% 298.81MB 0.63% github.com/nsqio/go-nsq.ReadResponse /home/vagrant/go/src/github.com/nsqio/go-nsq/protocol.go 2MB 0.0042% 99.15% 12097.28MB 25.41% main.(*PostPublisher).Publish /home/vagrant/logger/src/handler.go 1.50MB 0.0032% 99.15% 26358.53MB 55.37% io/ioutil.readAll /usr/local/go/src/io/ioutil/ioutil.go 1MB 0.0021% 99.16% 26378.74MB 55.41% github.com/gin-gonic/gin.LoggerWithWriter.func1 /home/vagrant/go/src/github.com/gin-gonic/gin/logger.go 0.50MB 0.0011% 99.16% 26434.42MB 55.53% net/http.(*conn).serve /usr/local/go/src/net/http/server.go 0 0% 99.16% 26357.03MB 55.36% bytes.(*Buffer).ReadFrom /usr/local/go/src/bytes/buffer.go(pprof) --alloc_objects 分析临时内存1go tool pprof -inuse_space http://127.0.0.1:4500/debug/pprof/heap 12345678910111213141516171819202122~ # go tool pprof -inuse_space http://127.0.0.1:4500/debug/pprof/heapFetching profile over HTTP from http://127.0.0.1:4500/debug/pprof/heapSaved profile in /home/vagrant/pprof/pprof.logger.alloc_objects.alloc_space.inuse_objects.inuse_space.007.pb.gzFile: loggerType: inuse_spaceTime: Jan 19， 2018 at 2:24pm (CST)Entering interactive mode (type &quot;help&quot; for commands， &quot;o&quot; for options)(pprof) topShowing nodes accounting for 20441.23kB， 100% of 20441.23kB totalShowing top 10 nodes out of 35 flat flat% sum% cum cum%18071.75kB 88.41% 88.41% 18071.75kB 88.41% bytes.makeSlice /usr/local/go/src/bytes/buffer.go 815.27kB 3.99% 92.40% 815.27kB 3.99% github.com/nsqio/go-diskqueue.(*diskQueue).readOne /home/vagrant/go/src/github.com/nsqio/go-diskqueue/diskqueue.go 528.17kB 2.58% 94.98% 528.17kB 2.58% regexp.(*bitState).reset /usr/local/go/src/regexp/backtrack.go 514kB 2.51% 97.50% 514kB 2.51% net/http.newBufioWriterSize /usr/local/go/src/bufio/bufio.go 512.05kB 2.50% 100% 512.05kB 2.50% net/http.(*persistConn).roundTrip /usr/local/go/src/net/http/transport.go 0 0% 100% 528.17kB 2.58% _/home/vagrant/logger/src/parser.ParserLogForMarco /home/vagrant/logger/src/parser/parser.go 0 0% 100% 528.17kB 2.58% bytes.(*Buffer).ReadFrom /usr/local/go/src/bytes/buffer.go 0 0% 100% 17543.58kB 85.82% bytes.(*Buffer).Write /usr/local/go/src/bytes/buffer.go 0 0% 100% 17543.58kB 85.82% bytes.(*Buffer).grow /usr/local/go/src/bytes/buffer.go 0 0% 100% 528.17kB 2.58% github.com/gin-gonic/gin.(*Context).Next /home/vagrant/go/src/github.com/gin-gonic/gin/context.go(pprof) 通过常驻内存和临时内存分配 top 值，可以查看当前程序的内存占用情况和热点内存使用的代码，结合代码分析热点代码是否存在 bug、是否有优化的空间。 go-torch通过上面的 go tool pprof 工具和 top 指令，我们能定位出程序的热点代码，但缺乏对程序运行情况的整体感知，能不能有类似火焰图的效果让我们对整个堆栈统计信息有个一目了然的效果呢？这里要感谢 uber 开源的工具 go-torch，能让我们将 profile 信息转换成火焰图，具体安装和使用过程见项目的介绍。 安装好 go-torch 后，运行 1go-torch -u http://127.0.0.1:4500 生成 CPU 火焰图 从图中能一眼看到 publish 函数中的压缩操作占了 70% 左右的 CPU。 而 gin httpserver 只占用了 2% 左右的 CPU， 和我们使用 go tool pprof 的 top 命令分析的结果一致。 默认情况下 go-torch 采集的是 CPU 的 profile， 这里介绍下 mem 火焰图的采集。 inuse_space 火焰图1go-torch -inuse_space http://127.0.0.1:4500/debug/pprof/heap --colors=mem alloc_space 火焰图1go-torch -alloc_space http://127.0.0.1:4500/debug/pprof/heap --colors=mem logger 100% CPU 分析前面介绍了 go tool pprof 和火焰图的使用方法，这里使用火焰图复现 logger 100% CPU 问题。 先看现象， 用 wrk 压测 logger 1wrk -t1 -c100 -d30 --script=post.lua &apos;http://127.0.0.1:4500/marco/log&apos; 查看 CPU 占用情况 采集 30s 的 CPU profile 火焰图 图中红色标记部分 startSink 函数中 runtime.selectgo 消耗了大量 CPU， 而 runtime.selectgo 上面只有 runtime.sellock 和 runtime.selunlock 两个操作，即大量 CPU 耗费在 select 操作上，火焰图呈秃顶状态，即瓶颈所在。 查看 startSink 实现 1234567891011121314151617181920212223242526272829303132333435363738394041for &#123; if exit == true &#123; return &#125; if moveforward &#123; fakeRead = readChan &#125; else &#123; fakeRead = nil &#125; select &#123; case read := &lt;-fakeRead: count++ buf.Write(read) case &lt;-done: DiskQueue.Close() exit = true default: //pass &#125; if count == GlobalConf.CntBatch || exit == true &#123; hostPoolResponse := pool.Get() addr := hostPoolResponse.Host() err := handler.Publish(fmt.Sprintf(&quot;%s%s&quot;， addr， SinkConf.Uri)， buf.Bytes()) hostPoolResponse.Mark(err) if err != nil &#123; Log.Error(&quot;%s&quot;， err.Error()) moveforward = false time.Sleep(1 * time.Second) continue &#125; else &#123; moveforward = true &#125; buf.Reset() count = 0 &#125;&#125; 本希望通过 moveforward 来控制 fakeRead 是否取值，而如果 fakeRead 为 nil 时， 整个 select 会一直阻塞，所以加上了 default 操作，让 select 变成非阻塞，但因为一直没有读取内容，count 没有增加而不会触发 sleep 操作。最终导致非阻塞的 select 一直空转循环，类似一个空 while 循环，占用了大量 CPU。 优化改用其他方法实现这部分逻辑，这里不再贴出来了，重在分享发现问题的过程，改进后的火焰图在前面已给出。 总结Golang 应用通常只要能编译通过，很少有运行时问题；而当应用遇到高CPU 、高内存占用或者作为 http 服务端响应时间长，QPS 上不去等，且不能 code review 解决时，可以尝试使用pprof 和 Flame-Graph 来分析定位问题，有奇效。当然 Golang 程序的调试及调优还有很多方法，比如直接结合go test 和 benchmark通过测用例分析热点代码、使用go pprof分析汇编代码等。]]></content>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 使用 grep 找回误删文件]]></title>
    <url>%2F2016%2F09%2F26%2Ffind-mistake-deleted-file-by-grep%2F</url>
    <content type="text"><![CDATA[在操作 linux 过程中，经常由于错误的操作误删除文件，则可以使用 grep 操作找回 命令1$ grep -a -B 10 -A 100 'vpsee.com' /dev/sda1 &gt; tmp.txt -a表示将磁盘 /dev/sda1 当做二进制文件来读 -B 10 -A 100 表示如果匹配到 vpsee.com，则打印前 10 行和后 100 行 匹配结果重定向到 tmp.txt note 在发现误删除文件后，尽量减少其他的文件操作避免已删除的文件被覆盖。 grep 操作耗费会占用大量的内存空间，如果发现 grep 失败（虚拟机中），可多分配内存重试。 可能删除的文件不在 /dev/sda1 磁盘上，可使用 df 来查看是否有其他磁盘。]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lua and 和 or 踩坑]]></title>
    <url>%2F2016%2F09%2F25%2Flua-and-or%2F</url>
    <content type="text"><![CDATA[lua and or 操作 a and b如果a为false，则返回 a；否则返回 b a or b如果 a 为 true，则返回 a；否则返回 b C语言中的语句：x = a? b : c，在Lua中可以写成：x = a and b or c 存在的问题12debug = 0(or 1)step = debug and 0 or 80 则无论 debug 取 0 还是 1，step 输出都是 0 当 debug 为0 120 and 0 = 00 or 80 = 0 当 debug 为 1 120 and 0 = 00 or 80 = 0 错误原因lua将 0 认为是 true 解决方法定义 debug = true or false1step = debug and 0 or 80 则当 debug 为 false 时候，step 为 80]]></content>
      <tags>
        <tag>Lua</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux core 设置]]></title>
    <url>%2F2016%2F09%2F25%2Fcore-dump-setting%2F</url>
    <content type="text"><![CDATA[linux 程序异常退出时，内核会生成 core 文件，通过 GDB 查看 core 文件可以定位程序异常退出时候的堆栈信息，指示出错的位置。 查看 core 文件是否打开1$ ulimit -c 如果结果为0，则表示系统关闭了该功能 开启 core在当前 shell 设置1$ ulimit -c unlimited 为整个用户设置1$ cat ulimit-c unlimited &gt; ~/.zshrc &amp;&amp; source ~/.zshrc core 设置文件名是否带pid标识1$ echo "1" &gt; /proc/sys/kernel/core_uses_pid core 文件格式为 corename_with_format.pid 1$ echo "0" &gt; /proc/sys/kernel/core_uses_pid core 文件格式为 corename_with_format 保存位置及格式设置查看当前设置1$ cat /proc/sys/kernel/core_pattern 位置及格式1$ echo "/corefile_path/core-%e-%p-%t" &gt; /proc/sys/kernel/core_pattern 具体参数含义 参数 描述 %p pid %u current uid %g current gid %s signal that caused the coredump %t UNIX time that the coredump occurred %h hostname where the coredump happened %e coredumping executable name]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo Qucik Start]]></title>
    <url>%2F2016%2F09%2F20%2Fhexo-quick-start%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
